@article{Reference1,
	Abstract = {What is computational modeling? Computational modeling is the use of computers to simulate and study complex systems using mathematics, physics and computer science. A computational model contains numerous variables that characterize the system being studied. Simulation is done by adjusting the variables alone or in combination and observing the outcomes. Computer modeling allows scientists to conduct thousands of simulated experiments by computer. The thousands of computer experiments identify the handful of laboratory experiments that are most likely to solve the problem being studied.Today’s computational models can study a biological system at multiple levels. Models of how disease develops include molecular processes, cell to cell interactions, and how those changes affect tissues and organs. Studying systems at multiple levels is known as multiscale modeling (MSM).},
	Journal = {National Institute of Biomedical Imaging and Bioengineering},
	Month = {May},
	Pages = {1},
	Title = {Computational Modeling},
	Url = {https://www.nibib.nih.gov/science-education/science-topics/computational-modeling},
	Year = {2020}}
	
@article{Reference2,
	title = {Test case generation for agent-based models: A systematic literature review},
	journal = {Information and Software Technology},
	volume = {135},
	pages = {106567},
	year = {2021},
	issn = {0950-5849},
	doi = {https://doi.org/10.1016/j.infsof.2021.106567},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584921000501},
	author = {Andrew G. Clark and Neil Walkinshaw and Robert M. Hierons},
	keywords = {Agent-based modelling, Multi-agent systems, Software testing, Test case generation, Systematic literature review},
	abstract = {Context:
	Agent-based models play an important role in simulating complex emergent phenomena and supporting critical decisions. In this context, a software fault may result in poorly informed decisions that lead to disastrous consequences. The ability to rigorously test these models is therefore essential.
	Objective:
	Our objective is to summarise the state-of-the-art techniques for test case generation in agent-based models and identify future research directions.
	Method:
	We have conducted a systematic literature review in which we pose five research questions related to the key aspects of test case generation in agent-based models: What are the information artifacts used to generate tests? How are these tests generated? How is a verdict assigned to a generated test? How is the adequacy of a generated test suite measured? What level of abstraction of an agent-based model is targeted by a generated test?
	Results:
	Out of the 464 initial search results, we identified 24 primary publications. Based on these primary publications, we formed a taxonomy to summarise the state-of-the-art techniques for test case generation in agent-based models. Our results show that whilst the majority of techniques are effective for testing functional requirements at the agent and integration levels of abstraction, there are comparatively few techniques capable of testing society-level behaviour. Furthermore, the majority of techniques cannot test non-functional requirements or “soft goals”.
	Conclusions:
	This paper reports insights into the key developments and open challenges concerning test case generation in agent-based models that may be of interest to both researchers and practitioners. In particular, we identify the need for test case generation techniques that focus on societal and non-functional behaviour, and a more thorough evaluation using realistic case studies that feature challenging properties associated with a typical agent-based model.}
	}

@article{Reference3,
	Abstract = {Computational modeling of behavior has revolutionized psychology and neuroscience. By fitting models to experimental data we can probe the algorithms underlying behavior, find neural correlates of computational variables and better understand the effects of drugs, illness and interventions. But with great power comes great responsibility. Here, we offer ten simple rules to ensure that computational modeling is used with care and yields meaningful insights. In particular, we present a beginner-friendly, pragmatic and details-oriented introduction on how to relate models to data. What, exactly, can a model tell us about the mind? To answer this, we apply our rules to the simplest modeling techniques most accessible to beginning modelers and illustrate them with examples and code available online. However, most rules apply to more advanced techniques. Our hope is that by following our guidelines, researchers will avoid many pitfalls and unleash the power of computational modeling on their own data.},
	Author = {Robert C Wilson, Anne GE Collins},
	Journal = {eLife},
	Month = {November},
	Numpages = {1},
	Pages = {1},
	Title = {Ten simple rules for the computational modeling of behavioral data},
	Url = {https://elifesciences.org/articles/49547#content},
	Year = {2019}}

@article{Reference4,
	Abstract = {Covasim is a stochastic agent-based simulator for performing COVID-19 analyses. These include projections of indicators such as numbers of infections and peak hospital demand. Covasim can also be used to explore the potential impact of different interventions, including social distancing, school closures, testing, contact tracing, quarantine, and vaccination.},
	Author = {Kerr CC, Stuart RM, Mistry D, etc.},
	Title = {Covasim},
	Url = {https://github.com/InstituteforDiseaseModeling/covasim},
	Year = {2021}}

@article{Reference5,
	title = {All Models Are Not Born Equal - Empirical vs. Mechanistic Models - Creme Global},
	author = {Creme Global},
	URL = {https://www.cremeglobal.com/explaining-empirical-and-mechanistic-models/}}

@article{Reference6,
	author = {Eric Bonabeau },
	title = {Agent-based modeling: Methods and techniques for simulating human systems},
	journal = {Proceedings of the National Academy of Sciences},
	volume = {99},
	number = {suppl\_3},
	pages = {7280-7287},
	year = {2002},
	doi = {10.1073/pnas.082080899},
	URL = {https://www.pnas.org/doi/abs/10.1073/pnas.082080899},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.082080899},
	abstract = {Agent-based modeling is a powerful simulation modeling technique that has seen a number of applications in the last few years, including applications to real-world business problems. After the basic principles of agent-based simulation are briefly introduced, its four areas of application are discussed by using real-world applications: flow simulation, organizational simulation, market simulation, and diffusion simulation. For each category, one or several business applications are described and analyzed.}}

@article{Reference7,
		author="Van Dyke Parunak, H.
		and Savit, Robert
		and Riolo, Rick L.",
		editor="Sichman, Jaime Sim{\~a}o
		and Conte, Rosaria
		and Gilbert, Nigel",
		title="Agent-Based Modeling vs. Equation-Based Modeling: A Case Study and Users' Guide",
		booktitle="Multi-Agent Systems and Agent-Based Simulation",
		year="1998",
		publisher="Springer Berlin Heidelberg",
		address="Berlin, Heidelberg",
		pages="10--25",
		abstract="In many domains, agent-based system modeling competes with equation-based approaches that identify system variables and evaluate or integrate sets of equations relating these variables. The distinction has been of great interest in a project that applies agent-based modeling to industrial supply networks, since virtually all computer-based modeling of such networks up to this point has used system dynamics, an approach based on ordinary differential equations (ODE's). This paper summarizes the domain of supply networks and illustrates how they can be modeled both with agents and with equations. It summarizes the similarities and differences of these two classes of models, and develops criteria for selecting one or the other approach.",
		isbn="978-3-540-49246-7"}

@article{Reference8,
	Abstract = {In order to deal with an increasingly complex world, we need ever more sophisticated computational models that can help us make decisions wisely and understand the potential consequences of choices. But creating a model requires far more than just raw data and technical skills: it requires a close collaboration between model commissioners, developers, users and reviewers. Good modelling requires its users and commissioners to understand more about the whole process, including the different kinds of purpose a model can have and the different technical bases. This paper offers a guide to the process of commissioning, developing and deploying models across a wide range of domains from public policy to science and engineering. It provides two checklists to help potential modellers, commissioners and users ensure they have considered the most significant factors that will determine success. We conclude there is a need to reinforce modelling as a discipline, so that misconstruction is less likely; to increase understanding of modelling in all domains, so that the misuse of models is reduced; and to bring commissioners closer to modelling, so that the results are more useful.},
	Author = {Muffy Calder, Claire Craig, etc.},
	Journal = {Royal Society Open Science},
	Month = {June},
	Numpages = {1},
	Title = {Computational modelling for decision-making: where, why, what, who and how},
	Url = {https://royalsocietypublishing.org/doi/10.1098/rsos.172096},
	Year = {2018}}

@article{Reference9,
	Abstract = {The COVID-19 pandemic has created an urgent need for models that can project epidemic trends, explore intervention scenarios, and estimate resource needs. Here we describe the methodology of Covasim (COVID-19 Agent-based Simulator), an open-source model developed to help address these questions. Covasim includes country-specific demographic information on age structure and population size; realistic transmission networks in different social layers, including households, schools, workplaces, long-term care facilities, and communities; age-specific disease outcomes; and intrahost viral dynamics, including viral-load-based transmissibility. Covasim also supports an extensive set of interventions, including non-pharmaceutical interventions, such as physical distancing and protective equipment; pharmaceutical interventions, including vaccination; and testing interventions, such as symptomatic and asymptomatic testing, isolation, contact tracing, and quarantine. These interventions can incorporate the effects of delays, loss-to-follow-up, micro-targeting, and other factors. Implemented in pure Python, Covasim has been designed with equal emphasis on performance, ease of use, and flexibility: realistic and highly customized scenarios can be run on a standard laptop in under a minute. In collaboration with local health agencies and policymakers, Covasim has already been applied to examine epidemic dynamics and inform policy decisions in more than a dozen countries in Africa, Asia-Pacific, Europe, and North America.},
	Author = {Cliff C. Kerr ,Robyn M. Stuart , etc.},
	Journal = {PLOS Computational Biology},
	Month = {July},
	Title = {Covasim: An agent-based model of COVID-19 dynamics and interventions},
	Url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009149},
	Year = {2018}}

@article{Reference10,
	Abstract = {Scientific software plays an important role in critical decision making, for example making weather predictions based on climate models, and computation of evidence for research publications. Recently, scientists have had to retract publications due to errors caused by software faults. Systematic testing can identify such faults in code.},
	Author = {Upulee Kanewala, James M.Bieman},
	Journal = {ScienceDirect},
	Month = {May},
	Title = {Testing scientific software: A systematic literature review},
	Url = {https://www.sciencedirect.com/science/article/pii/S0950584914001232},
	Year = {2014}}

@article{Reference11,
	Abstract = {The two cultures of the title are those observed in my field studies: the culture of scientists (financial mathematicians, earth and planetary scientists, and molecular biologists) developing their own software, and the culture of software engineers developing scientific software. In this paper, I shall describe some problems arising when scientists and software engineers come together to develop scientific software and discuss how these problems may be ascribed to their two different cultures.},
	Author = {Judith Segal},
	Journal = {Open research Online},
	Number = {1},
	Numpages = {8},
	Pages = {2},
	Title = {Scientists and software engineers: A tale of two cultures},
	Url = {http://oro.open.ac.uk/17671/1/PPIG_08Segal.pdf},
	Year = {2008}}

@article{Reference12,
	title={Validating Computational Models},
	author={Kathleen M. Carley},
	Url = {}
	year={1996}
}

@article{Reference13,
	Abstract = {Behavior-driven development (or BDD) is an agile software development technique that encourages collaboration between developers, QA and non-technical or business participants in a software project. It was originally named in 2003 by Dan North as a response to test-driven development (TDD), including acceptance test or customer test driven development practices as found in extreme programming. It has evolved over the last few years.},
	Author = {Benno Rice, Richard Jones and Jens Engel},
	Title = {Behavior Driven Development},
	Url = {https://behave.readthedocs.io/en/stable/philosophy.html},
	Year = {2017}}

@article{Reference14,
	Abstract = {Cucumber reads executable specifications written in plain text and validates that the software does what those specifications say. The specifications consists of multiple examples, or scenarios.},
	Author = {Marit van Dijk, Aslak Hellesoy, etc.},
	Title = {Cucumber},
	Url = {https://cucumber.io/docs/guides/overview/},
	Year = {2019}}

@article{Reference15,
	Abstract = {Gherkin uses a set of special keywords to give structure and meaning to executable specifications. Each keyword is translated to many spoken languages; in this reference we’ll use English.},
	Author = {Marit van Dijk, Aslak Hellesoy, etc.},
	Title = {Gherkin Reference},
	Url = {https://cucumber.io/docs/gherkin/reference/},
	Year = {2019}}

@article{Reference16,
	Abstract = {Causal models are mathematical models representing causal relationships within an individual system or population. They facilitate inferences about causal relationships from statistical data. They can teach us a good deal about the epistemology of causation, and about the relationship between causation and probability. They have also been applied to topics of interest to philosophers, such as the logic of counterfactuals, decision theory, and the analysis of actual causation.},
	Author = {Christopher Hitchcock},
	Title = {Causal Models},
	Url = {https://plato.stanford.edu/entries/causal-models/},
	Year = {2018}}

@article{Reference17,
	Abstract = {Understanding the root cause of a defect is critical to isolating and repairing buggy behavior. We present Causal Testing, a new method of root-cause analysis that relies on the theory of counterfactual causality to identify a set of executions that likely hold key causal information necessary to understand and repair buggy behavior. Using the Defects4J benchmark, we find that Causal Testing could be applied to 71% of real-world defects, and for 77% of those, it can help developers identify the root cause of the defect. A controlled experiment with 37 developers shows that Causal Testing improves participants' ability to identify the cause of the defect from 80% of the time with standard testing tools to 86% of the time with Causal Testing. The participants report that Causal Testing provides useful information they cannot get using tools such as JUnit. Holmes, our prototype, open-source Eclipse plugin implementation of Causal Testing, is available at this http URL.},
	Author = {Brittany Johnson, Yuriy Brun, Alexandra Meliou},
	Title = {Causal Testing: Finding Defects' Root Causes},
	Pages = {123 - 146},
	Month = {Feb},
	Url = {https://plato.stanford.edu/entries/causal-models/},
	Year = {2020}}

@article{Reference18,
	author = {Johnson, Brittany and Brun, Yuriy and Meliou, Alexandra},
	title = {Causal Testing: Understanding Defects' Root Causes},
	year = {2020},
	isbn = {9781450371216},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3377811.3380377},
	doi = {10.1145/3377811.3380377},
	abstract = {Understanding the root cause of a defect is critical to isolating and repairing buggy behavior. We present Causal Testing, a new method of root-cause analysis that relies on the theory of counterfactual causality to identify a set of executions that likely hold key causal information necessary to understand and repair buggy behavior. Using the Defects4J benchmark, we find that Causal Testing could be applied to 71% of real-world defects, and for 77% of those, it can help developers identify the root cause of the defect. A controlled experiment with 37 developers shows that Causal Testing improves participants' ability to identify the cause of the defect from 80% of the time with standard testing tools to 86% of the time with Causal Testing. The participants report that Causal Testing provides useful information they cannot get using tools such as JUnit. Holmes, our prototype, open-source Eclipse plugin implementation of Causal Testing, is available at http://holmes.cs.umass.edu/.},
	booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
	pages = {87-99},
	numpages = {13},
	keywords = {theory of counterfactual causality, causal testing, causality, software debugging, Holmes, test fuzzing, automated test generation},
	location = {Seoul, South Korea},
	series = {ICSE '20}
}

@article{Reference19,
	Abstract = {Problem: Partial least squares (PLS), a form of structural equation modeling (SEM), can provide much value for causal inquiry in communication-related and behavioral research fields. Despite the wide availability of technical information on PLS, many behavioral and communication researchers often do not use PLS in situations in which it could provide unique theoretical insights. Moreover, complex models comprising formative (causal) and reflective (consequent) constructs are now common in behavioral research, but they are often misspecified in statistical models, resulting in erroneous tests. Key concepts: First-generation (1G) techniques, such as correlations, regressions, or difference of means tests (such as ANOVA or t-tests), offer limited modeling capabilities, particularly in terms of causal modeling. In contrast, second-generation techniques (such as covariance-based SEM or PLS) offer extensive, scalable, and flexible causal-modeling capabilities. Second-generation (2G) techniques do not invalidate the need for 1G techniques however. The key point of 2G techniques is that they are superior for the complex causal modeling that dominates recent communication and behavioral research. Key lessons: For exploratory work, or for studies that include formative constructs, PLS should be selected. For confirmatory work, either covariance-based SEM or PLS may be used. Despite claims that lower sampling requirements exist for PLS, inadequate sample sizes result in the same problems for either technique. Implications: SEM's strength is in modeling. In particular, SEM allows for complex models that include latent (unobserved) variables, formative variables, chains of effects (mediation), and multiple group comparisons of these more complex relationships.},
	Author = {Paul Benjamin Lowry; James Gaskin},
	Title = {Partial Least Squares (PLS) Structural Equation Modeling (SEM) for Building and Testing Behavioral Causal Theory: When to Choose It and How to Use It},
	Month = {April},
	Url = {https://ieeexplore.ieee.org/abstract/document/6803892/authors#authors},
	Year = {2014}}

@article{Reference20,
	Author = {Kivy.org.},
	Title = {Introduction — Kivy 2.1.0 documentation},
	Url = {https://kivy.org/doc/stable/gettingstarted/intro.html},
	Year = {2022}}

@article{Reference21,
	Author = {Foster, M., Clark, A., Turner, R. and Wild, C.},
	Title = {CITCOM-project/causcumber},
	Url = {https://github.com/CITCOM-project/causcumber},
	Year = {2022}}

@article{Reference22,
	author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
				Haberland, Matt and Reddy, Tyler and Cournapeau, David and
				Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
				Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
				Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
				Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
				Kern, Robert and Larson, Eric and Carey, C J and
				Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
				{VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
				Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
				Harris, Charles R. and Archibald, Anne M. and
				Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
				{van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
	title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
				Computing in Python}},
	journal = {Nature Methods},
	year    = {2020},
	volume  = {17},
	pages   = {261--272},
	adsurl  = {https://rdcu.be/b08Wh},
	doi     = {10.1038/s41592-019-0686-2},
}

@article{Reference23,
	Author = {Progclub.org.},
	Title = {Metasyntactic variable.},
	Url = {https://www.progclub.org/pipermail/programming/2016-November/002305.html},
	Year = {2022}}

@article{Reference24,
  author={Sukumar, Sreenivas R. and Nutaro, James J.},
  booktitle={2012 ASE/IEEE International Conference on BioMedical Computing (BioMedCom)}, 
  title={Agent-Based vs. Equation-Based Epidemiological Models: A Model Selection Case Study}, 
  year={2012},
  volume={},
  number={},
  pages={74-79},
  doi={10.1109/BioMedCom.2012.19}}

